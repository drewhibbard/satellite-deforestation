# -*- coding: utf-8 -*-
"""CNN_VGG16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1czmWMTwWgvPgMKclQs43lKqwi4DzFXQE
"""

# !pip install h5imagegenerator

import keras
import numpy as np
from keras.models import Sequential
from keras.utils import np_utils
from keras.layers import InputLayer, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Flatten, Dense, BatchNormalization
from keras.preprocessing import image
from keras.applications import vgg16
from keras import Model
import tensorflow as tf
from sklearn.model_selection import train_test_split
import PIL
import os
import cupy as cp
import pickle
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import multilabel_confusion_matrix, classification_report, confusion_matrix
import h5py
from h5imagegenerator import HDF5ImageGenerator

from google.colab import drive
drive.mount('/content/drive')

os.chdir('drive/My Drive/Metis/satellite_deforestation')

import sys
sys.path.append('modules/')
import custom_metrics as cm

with open('data/tags.pickle','rb') as rf:
  tags = pickle.load(rf)

# weight the weather tags less than the others because they won't help find deforestation
class_weights = {0: 0.75, 1: 1.0, 2: 0.75, 3: 0.5, 4: 0.5, 5: 0.5, 6: 0.5, 7: 1.0, 8: 1.0, 9: 1.0,
                 10: 0.5, 11: 0.5, 12: 0.75, 13: 1.0, 14: 1.0, 15: 1.0, 16: 0.75}

# data won't fit in memory, so create a generator and batch-feed it to the model
train_generator = HDF5ImageGenerator(src='data/tensors_224.h5',
                                     X_key='X_train',
                                     y_key='y_train',
                                     labels_encoding=False,
                                     scaler=True,
                                     batch_size=128,
                                     mode='train')

test_generator = HDF5ImageGenerator(src='data/tensors_224.h5',
                                    X_key='X_test',
                                    y_key='y_test',
                                    labels_encoding=False,
                                    scaler=True,
                                    batch_size=128,
                                    mode='test')

base_model = vgg16.VGG16(weights='imagenet',include_top=False,input_shape=(224,224,3))
input_shape = keras.Input(shape=(224,224,3))

# turn off training for the convolutional layers
for layer in base_model.layers:
  layer.trainable = False

normalized = BatchNormalization()(input_shape)
input_layer = base_model(normalized)
input_layer = Flatten()(input_layer)
# input_layer = Dense(30,activation='relu')(input_layer)
# 17 classes so 17 output layers, sigmoid because we want each class to be treated as a separate prediction
predictions = Dense(17,activation='sigmoid')(input_layer) 

CNN = Model(input_shape,predictions)
# even though multi-class, we use binary cross-entropy because each class should be a separate prediction
CNN.compile(optimizer='adam',loss='binary_crossentropy',metrics=[cm.challenge_accuracy])

CNN.summary()

CNN.fit(train_generator,
        workers=2,
        verbose=1,
        epochs=30,
        use_multiprocessing=True,
        class_weight=class_weights,
        callbacks = [keras.callbacks.EarlyStopping(patience=3),
                     keras.callbacks.ReduceLROnPlateau(monitor='val_loss',
                                                       factor=0.5,
                                                       patience=2,
                                                       min_lr=0.0002),
                     keras.callbacks.ReduceLROnPlateau(monitor='val_loss',
                                                       factor=0.5,
                                                       patience=2,
                                                       verbose=1,
                                                       min_lr=0.0002)
                     ])

predictions = CNN.predict(test_generator,
        workers=2,
        verbose=1,
        use_multiprocessing=True)

f = h5py.File('data/tensors_224.h5','r') 
y_test = f['y_test'][:]

cm.challenge_score(y_test,predictions)

cm.item_accuracy(y_test,predictions)

cm.full_accuracy(y_test,predictions)

cm.multi_class_confusion(y_test,predictions,tags)

CNN.save('CNN_VGG16')

